{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchshow as ts\n",
    "from data.dali.module import DALIDataModule\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# sys.path.append('/home/whx/SRFormer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_LIST = ['msl', '2t', '10u', '10v', \n",
    "            'hgtn_5000', 'hgtn_10000', 'hgtn_15000', 'hgtn_20000', 'hgtn_25000', 'hgtn_30000', 'hgtn_40000', 'hgtn_50000', 'hgtn_60000', 'hgtn_70000', 'hgtn_85000', 'hgtn_92500', 'hgtn_100000', \n",
    "            'u_5000', 'u_10000', 'u_15000', 'u_20000', 'u_25000', 'u_30000', 'u_40000', 'u_50000', 'u_60000', 'u_70000', 'u_85000', 'u_92500', 'u_100000', \n",
    "            'v_5000', 'v_10000', 'v_15000', 'v_20000', 'v_25000', 'v_30000', 'v_40000', 'v_50000', 'v_60000', 'v_70000', 'v_85000', 'v_92500', 'v_100000', \n",
    "            't_5000', 't_10000', 't_15000', 't_20000', 't_25000', 't_30000', 't_40000', 't_50000', 't_60000', 't_70000', 't_85000', 't_92500', 't_100000', \n",
    "            'q_5000', 'q_10000', 'q_15000', 'q_20000', 'q_25000', 'q_30000', 'q_40000', 'q_50000', 'q_60000', 'q_70000', 'q_85000', 'q_92500', 'q_100000'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_file = 'configs/datasets/goes_hw64x64.txt'\n",
    "data_dir = {\n",
    "        \"hrrr\": 'hrrr/hourly2_fixed_TMP_L103',\n",
    "        \"era5\": 'era5_us_npy/npy',\n",
    "    }\n",
    "input_len, output_len = 1, 1\n",
    "time_config = {\n",
    "        'hrrr': {'input': input_len, 'output': output_len, 'gap': 1}, \n",
    "        'era5': {'input': input_len, 'output': output_len, 'gap': 1}, \n",
    "    }\n",
    "crop_h= 32\n",
    "crop_w= 32\n",
    "crop_cnt= 1\n",
    "batch_size= 4\n",
    "pad= 0\n",
    "margin= 0\n",
    "h= 105\n",
    "w= 179\n",
    "mount_paths = ['/blob/kmsw0westus3/kms1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacty of 44.55 GiB of which 297.56 MiB is free. Process 2957527 has 12.26 GiB memory in use. Including non-PyTorch memory, this process has 31.93 GiB memory in use. Of the allocated memory 19.79 GiB is allocated by PyTorch, and 19.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m data_module\u001b[38;5;241m.\u001b[39mprepare()\n\u001b[1;32m      7\u001b[0m data_module\u001b[38;5;241m.\u001b[39msetup()\n\u001b[0;32m----> 8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m t_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var \u001b[38;5;129;01min\u001b[39;00m VAR_LIST:\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/module.py:511\u001b[0m, in \u001b[0;36mDALIDataModule.val_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mval_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EVAL_DATALOADERS:\n\u001b[1;32m    510\u001b[0m     backend\u001b[38;5;241m.\u001b[39mReleaseUnusedMemory()\n\u001b[0;32m--> 511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/module.py:484\u001b[0m, in \u001b[0;36mDALIDataModule._get_dataloader\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# npy_pipe.build()\u001b[39;00m\n\u001b[1;32m    483\u001b[0m data_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_paths[mode]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m--> 484\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mMultiDataIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpy_pipe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtimestamps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_timestampes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moutput_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtime_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcrop_cnt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_cnt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfold_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mposition_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcrop_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mcrop_w\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mmargin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mreader_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_names\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mlast_batch_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLastBatchPolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDROP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mprepare_first_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/module.py:223\u001b[0m, in \u001b[0;36mMultiDataIterator.__init__\u001b[0;34m(self, pipelines, timestamps, output_map, time_config, mode, crop_cnt, fold_ratio, position_file, crop_h, crop_w, h, w, margin, pad, overlap, size, reader_name, auto_reset, fill_last_batch, dynamic_shape, last_batch_padded, last_batch_policy, prepare_first_batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pipelines:Pipeline, timestamps, output_map, time_config, mode,  \n\u001b[1;32m    217\u001b[0m              crop_cnt\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, fold_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, position_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, crop_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, crop_w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[1;32m    218\u001b[0m              h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m105\u001b[39m, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m179\u001b[39m, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, pad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m              last_batch_padded\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, last_batch_policy\u001b[38;5;241m=\u001b[39mLastBatchPolicy\u001b[38;5;241m.\u001b[39mFILL, \n\u001b[1;32m    222\u001b[0m              prepare_first_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mfill_last_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_batch_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlast_batch_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepare_first_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestamps \u001b[38;5;241m=\u001b[39m timestamps\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/iterator.py:86\u001b[0m, in \u001b[0;36mDALIGenericIterator.__init__\u001b[0;34m(self, pipelines, output_map, size, reader_name, auto_reset, fill_last_batch, dynamic_shape, last_batch_padded, last_batch_policy, prepare_first_batch)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_first_batch:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_batch \u001b[38;5;241m=\u001b[39m \u001b[43mDALIGenericIterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;66;03m# call to `next` sets _ever_consumed to True but if we are just calling it from\u001b[39;00m\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;66;03m# here we should set if to False again\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ever_consumed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/iterator.py:151\u001b[0m, in \u001b[0;36mDALIGenericIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m         pyt_tensors[category] \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mempty(c, \n\u001b[1;32m    147\u001b[0m                                              dtype\u001b[38;5;241m=\u001b[39mcategory_torch_type[category], \n\u001b[1;32m    148\u001b[0m                                              device\u001b[38;5;241m=\u001b[39mcategory_device[category]) \n\u001b[1;32m    149\u001b[0m                                  \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m category_shapes[category]]\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m         pyt_tensors[category] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_shapes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_torch_type\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategory_device\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m data_batches[i] \u001b[38;5;241m=\u001b[39m pyt_tensors\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Copy data from DALI Tensors to torch tensors\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacty of 44.55 GiB of which 297.56 MiB is free. Process 2957527 has 12.26 GiB memory in use. Including non-PyTorch memory, this process has 31.93 GiB memory in use. Of the allocated memory 19.79 GiB is allocated by PyTorch, and 19.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "data_module = DALIDataModule(data_dir=data_dir, batch_size=batch_size, num_threads=8, \n",
    "                            time_config=time_config, mount_paths=mount_paths, \n",
    "                            crop_cnt=crop_cnt, crop_h=crop_h, crop_w=crop_w, h=h, w=w, \n",
    "                            margin=margin, pad=pad, train_end_datetime='2020123100')\n",
    "\n",
    "data_module.prepare()\n",
    "data_module.setup()\n",
    "dataloader = data_module.val_dataloader()\n",
    "\n",
    "t_dict = {}\n",
    "for var in VAR_LIST:\n",
    "    t_dict[var] = {}\n",
    "    for t in range(input_len + output_len):\n",
    "        t_dict[var][t] = {'mean': [], 'std': []}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[4, 1, -1, 40, 40]' is invalid for input of size 266240",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_hrrr\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_era5\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/amlt9/lib/python3.8/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/module.py:294\u001b[0m, in \u001b[0;36mMultiDataIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m    293\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/module.py:208\u001b[0m, in \u001b[0;36mCropIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_cnt:\n\u001b[0;32m--> 208\u001b[0m         d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rand_crop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[0;32m~/pl_SRFormer/data/dali/module.py:156\u001b[0m, in \u001b[0;36mCropIterator._rand_crop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m     b, t \u001b[38;5;241m=\u001b[39m cropped_data\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    155\u001b[0m     single \u001b[38;5;241m=\u001b[39m cropped_data[:, :, :single_len]\n\u001b[0;32m--> 156\u001b[0m     atmos \u001b[38;5;241m=\u001b[39m \u001b[43mcropped_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msingle_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_h\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop_w\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     batch[k] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurface\u001b[39m\u001b[38;5;124m'\u001b[39m: single,\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpressure\u001b[39m\u001b[38;5;124m'\u001b[39m: atmos\n\u001b[1;32m    162\u001b[0m     }\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhrrr\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m k:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[4, 1, -1, 40, 40]' is invalid for input of size 266240"
     ]
    }
   ],
   "source": [
    "for batch in tqdm(dataloader):\n",
    "    print(batch['input_hrrr']['surface'].shape)\n",
    "\n",
    "    print(batch['input_era5']['surface'].shape)\n",
    "    # atmo_hrrr = batch['input_hrrr']['pressure'].reshape(-1, 1, 65, 1050, 1790).squeeze(1)\n",
    "    # data_hrrr = torch.cat([surf_hrrr, atmo_hrrr], dim=1)\n",
    "    # print(data_hrrr.shape)\n",
    "\n",
    "    # surf_era5 = batch['input_era5']['surface'].reshape(-1, 1, 4, 105, 179).squeeze(1)\n",
    "    # atmo_era5 = batch['input_era5']['pressure'].reshape(-1, 1, 65, 105, 179).squeeze(1)\n",
    "    # data_era5 = torch.cat([surf_era5, atmo_era5], dim=1)\n",
    "    # print(data_era5.shape)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New-0220"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os   \n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "variable_name=['msl', '2t', '10u', '10v', \n",
    "'hgtn_5000', 'hgtn_10000', 'hgtn_15000', 'hgtn_20000', 'hgtn_25000','hgtn_30000', 'hgtn_40000', 'hgtn_50000', 'hgtn_60000','hgtn_70000', 'hgtn_85000', 'hgtn_92500', 'hgtn_100000',\n",
    "'u_5000', 'u_10000', 'u_15000', 'u_20000', 'u_25000', 'u_30000', 'u_40000', 'u_50000', 'u_60000', 'u_70000', 'u_85000', 'u_92500','u_100000',\n",
    "'v_5000', 'v_10000', 'v_15000','v_20000','v_25000', 'v_30000','v_40000', 'v_50000', 'v_60000','v_70000','v_85000','v_92500','v_100000',\n",
    "'t_5000','t_10000','t_15000','t_20000','t_25000','t_30000','t_40000', 't_50000', 't_60000','t_70000','t_85000','t_92500','t_100000',\n",
    "'q_5000','q_10000','q_15000','q_20000','q_25000','q_30000','q_40000','q_50000','q_60000','q_70000','q_85000','q_92500','q_100000',]\n",
    "\n",
    "ALL_STDS = torch.Tensor([6.8789e+02, 1.1408e+01, 3.4087e+00, 3.8128e+00, 1.9669e+02, 2.2751e+02,\n",
    "2.8561e+02, 3.0650e+02, 2.9603e+02, 2.7141e+02, 2.1778e+02, 1.7208e+02,\n",
    "1.3423e+02, 1.0113e+02, 6.2827e+01, 5.5131e+01, 5.6151e+01, 9.4211e+00,\n",
    "1.0859e+01, 1.3950e+01, 1.7115e+01, 1.7562e+01, 1.6353e+01, 1.3276e+01,\n",
    "1.0801e+01, 8.9426e+00, 7.5022e+00, 6.1020e+00, 5.4538e+00, 3.7264e+00,\n",
    "4.3335e+00, 7.7346e+00, 1.2055e+01, 1.6379e+01, 1.7496e+01, 1.6438e+01,\n",
    "1.3244e+01, 1.0723e+01, 8.8519e+00, 7.4927e+00, 6.6357e+00, 6.3794e+00,\n",
    "4.4868e+00, 4.5961e+00, 6.9949e+00, 5.8210e+00, 4.5001e+00, 4.9734e+00,\n",
    "6.2033e+00, 7.2265e+00, 7.4370e+00, 7.7510e+00, 8.4697e+00, 9.9353e+00,\n",
    "1.0570e+01, 1.0831e+01, 1.0117e-06, 2.1395e-06, 4.7380e-06, 1.8255e-05,\n",
    "6.3045e-05, 1.5294e-04, 4.8075e-04, 9.9271e-04, 1.6142e-03, 2.2498e-03,\n",
    "3.5426e-03, 4.3892e-03, 5.0458e-03])\n",
    "\n",
    "ALL_MEANS = torch.Tensor([1.0154e+05,  2.8764e+02,  3.6334e-01, -2.1942e-01,  2.0653e+04,\n",
    "1.6401e+04,  1.3901e+04,  1.2086e+04,  1.0639e+04,  9.4158e+03,\n",
    "7.3890e+03,  5.7308e+03,  4.3219e+03,  3.0932e+03,  1.4998e+03,\n",
    "7.9164e+02,  1.2938e+02,  1.9525e+00,  1.3733e+01,  2.1062e+01,\n",
    "2.3357e+01,  2.1682e+01,  1.9152e+01,  1.4822e+01,  1.1510e+01,\n",
    "8.7861e+00,  6.3429e+00,  2.7486e+00,  1.2042e+00,  3.4059e-01,\n",
    "-1.9104e-01,  1.5638e-01,  3.5123e-01,  4.5588e-01,  4.4878e-01,\n",
    "3.9698e-01,  2.3847e-01,  1.6056e-01,  1.3170e-01,  1.8216e-01,\n",
    "3.9488e-01,  3.1365e-01, -2.5557e-01,  2.1192e+02,  2.0901e+02,\n",
    "2.1325e+02,  2.1837e+02,  2.2547e+02,  2.3356e+02,  2.4815e+02,\n",
    "2.5946e+02,  2.6831e+02,  2.7558e+02,  2.8359e+02,  2.8666e+02,\n",
    "2.9031e+02,  2.8537e-06,  3.0846e-06,  6.6055e-06,  2.4169e-05,\n",
    "7.3131e-05,  1.6582e-04,  4.9159e-04,  1.0437e-03,  1.8908e-03,\n",
    "2.9435e-03,  5.3577e-03,  7.2076e-03,  8.8479e-03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import datetime, timedelta\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import os \n",
    "# from datetime import datetime, timedelta\n",
    "# from copy import deepcopy\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# class HRRRDataset(Dataset):\n",
    "#     def __init__(self, file_names, root_dir, means, stds, device='cpu'):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             file_names (list of str): List of file names to be loaded.\n",
    "#             root_dir (str): Directory with all the .npy files.\n",
    "#             means (torch.Tensor): Tensor of means for normalization.\n",
    "#             stds (torch.Tensor): Tensor of standard deviations for normalization.\n",
    "#             device (str): Device to load the data onto, default is 'cpu'.\n",
    "#         \"\"\"\n",
    "#         self.file_names = file_names\n",
    "#         self.root_dir = root_dir\n",
    "#         self.means = means.to(device)\n",
    "#         self.stds = stds.to(device)\n",
    "#         self.device = device\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.file_names)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         file_path = os.path.join(self.root_dir, self.file_names[idx])\n",
    "#         data = np.load(file_path)\n",
    "#         data = torch.from_numpy(data).float().to(self.device)\n",
    "#         start_dim3 = (data.size(2) - 1050) // 2  \n",
    "#         end_dim3 = start_dim3 + 1050  \n",
    "#         start_dim4 = (data.size(3) - 1790) // 2  \n",
    "#         end_dim4 = start_dim4 + 1790  \n",
    "#         data = data[:, :, start_dim3:end_dim3, start_dim4:end_dim4]\n",
    "#         data = (data - self.means[:, None, None]) / self.stds[:, None, None]\n",
    "#         return data\n",
    "\n",
    "# def generate_file_names(start_date, end_date):\n",
    "#     \"\"\"\n",
    "#     Generates a list of file names for each hour between the start and end dates.\n",
    "#     \"\"\"\n",
    "#     file_names = []\n",
    "#     current_date = start_date\n",
    "#     while current_date <= end_date:\n",
    "#         for hour in range(24):\n",
    "#             file_names.append(current_date.strftime('%Y%m%d') + f'{hour:02d}.npy')\n",
    "#         current_date += timedelta(days=1)\n",
    "#     return file_names\n",
    "\n",
    "# # Set random seed for reproducibility\n",
    "# seed = 42\n",
    "# torch.manual_seed(seed)\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# # Define date range\n",
    "# start_date = datetime(2019, 1, 1)\n",
    "# end_date = datetime(2020, 12, 31)\n",
    "\n",
    "# # Generate file names\n",
    "# file_names = generate_file_names(start_date, end_date)\n",
    "\n",
    "# # Shuffle and split file names into training, testing, and validation sets\n",
    "# np.random.shuffle(file_names)\n",
    "# total_files = len(file_names)\n",
    "# train_size = int(total_files * 0.8)\n",
    "# test_size = int(total_files * 0.1)\n",
    "# val_size = total_files - train_size - test_size\n",
    "\n",
    "# train_files = file_names[:train_size]\n",
    "# test_files = file_names[train_size:train_size + test_size]\n",
    "# val_files = file_names[train_size + test_size:]\n",
    "\n",
    "\n",
    "# # Directory containing .npy files\n",
    "# hrrr_root = \"/blob/kmsw0westus3/kms1/hrrr/hourly2_fixed_TMP_L103\"  \n",
    "# device = 'cuda:1'\n",
    "\n",
    "# # Create dataset instances\n",
    "# train_dataset = HRRRDataset(train_files, hrrr_root, ALL_MEANS, ALL_STDS,device)\n",
    "# test_dataset = HRRRDataset(test_files, hrrr_root, ALL_MEANS, ALL_STDS,device)\n",
    "# val_dataset = HRRRDataset(val_files, hrrr_root, ALL_MEANS, ALL_STDS,device)\n",
    "\n",
    "# # Create DataLoader instances\n",
    "# batch_size = 64  # Adjust based on your system's capability\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将hrrr数据存成h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install h5py\n",
    "# import h5py\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# def preprocess_data(data):\n",
    "#     # 这里添加你的数据预处理代码，比如裁剪、规范化等\n",
    "#     start_dim3 = (data.shape[2] - 1050) // 2  \n",
    "#     end_dim3 = start_dim3 + 1050  \n",
    "#     start_dim4 = (data.shape[3] - 1790) // 2  \n",
    "#     end_dim4 = start_dim4 + 1790  \n",
    "#     data = data[:, :, start_dim3:end_dim3, start_dim4:end_dim4]\n",
    "#     return data\n",
    "\n",
    "# def save_to_hdf5(file_names, root_dir, output_file):\n",
    "#     with h5py.File(output_file, 'w') as h5f:\n",
    "#         for i, file_name in enumerate(file_names):\n",
    "#             file_path = os.path.join(root_dir, file_name)\n",
    "#             # 加载原始数据\n",
    "#             data = np.load(file_path)\n",
    "#             # 预处理数据\n",
    "#             data = preprocess_data(data)\n",
    "#             # 将数据存储到 HDF5 文件中\n",
    "#             h5f.create_dataset(name=file_name, data=data, compression=\"gzip\")\n",
    "#             print(f'Processed and saved {file_name} to {output_file}')\n",
    "\n",
    "# def generate_file_names(start_date, end_date):\n",
    "#     file_names = []\n",
    "#     current_date = start_date\n",
    "#     while current_date <= end_date:\n",
    "#         for hour in range(24):\n",
    "#             file_names.append(current_date.strftime('%Y%m%d') + f'{hour:02d}.npy')\n",
    "#         current_date += timedelta(days=1)\n",
    "#     return file_names\n",
    "\n",
    "# # 定义日期范围\n",
    "# start_date = datetime(2019, 1, 1)\n",
    "# end_date = datetime(2020, 12, 31)\n",
    "\n",
    "# # 生成文件名列表\n",
    "# file_names = generate_file_names(start_date, end_date)\n",
    "\n",
    "# # 定义数据根目录和输出文件路径\n",
    "# hrrr_root = \"/blob/kmsw0westus3/kms1/hrrr/hourly2_fixed_TMP_L103\"  # 更新为你的.npy文件的路径\n",
    "# output_h5_file = \"/home/whx/data/hrrr_hourly2_fixed_TMP_L103.h5\"  # 输出HDF5文件的路径\n",
    "\n",
    "# # 保存数据到 HDF5\n",
    "# save_to_hdf5(file_names, hrrr_root, output_h5_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrrr_root = \"/blob/kmsw0westus3/kms1/hrrr/hourly2_fixed_TMP_L103\"\n",
    "# for file in os.listdir(hrrr_root):\n",
    "#     print(file)\n",
    "# 2019-01-01 ~ 2020-12-31 每天24个npy\n",
    "\n",
    "year, month, day_start, day_end = 2019, 1, 1, 2 # 2020, 8, 9, 12 # 2019, 10, 26, 29\n",
    "# case_timestamp = f\"{year}_{month}_{day_start}_{day_end}\"\n",
    "device = 'cuda:1'\n",
    "start_time = datetime(year=year, month=month, day=day_start)\n",
    "end_time = datetime(year=year, month=month, day=day_end)\n",
    "dts = []\n",
    "while start_time < end_time:\n",
    "    dts.append(deepcopy(start_time))\n",
    "    start_time += timedelta(hours=1)\n",
    "data = []\n",
    "for d in tqdm(dts):\n",
    "    data.append(np.load(os.path.join(hrrr_root, d.strftime('%Y%m%d%H.npy'))))\n",
    "data = np.concatenate(data, axis=0)\n",
    "case_data = torch.from_numpy(data[:, :, :, :]).to(device)\n",
    "all_means = ALL_MEANS.to(device)\n",
    "all_stds = ALL_STDS.to(device)\n",
    "case_data = ((case_data - all_means[None,:,None,None]) / all_stds[None,:,None,None])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.28it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_means' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(data, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m era5_case_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(data[:, :, :, :])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 26\u001b[0m era5_case_data \u001b[38;5;241m=\u001b[39m ((era5_case_data \u001b[38;5;241m-\u001b[39m \u001b[43mall_means\u001b[49m[\u001b[38;5;28;01mNone\u001b[39;00m,:,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m]) \u001b[38;5;241m/\u001b[39m all_stds[\u001b[38;5;28;01mNone\u001b[39;00m,:,\u001b[38;5;28;01mNone\u001b[39;00m,\u001b[38;5;28;01mNone\u001b[39;00m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_means' is not defined"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "era5_root = \"/blob/kmsw0westus3/kms1/era5_us_npy/npy/\"\n",
    "# for file in os.listdir(era5_root):\n",
    "#     print(file)\n",
    "# 2019-01-01 ~ 2020-12-31 每天24个npy\n",
    "year, month, day_start, day_end = 2019, 1, 1, 2 # 2020, 8, 9, 12 # 2019, 10, 26, 29\n",
    "# case_timestamp = f\"{year}_{month}_{day_start}_{day_end}\"\n",
    "device = 'cuda:1'\n",
    "start_time = datetime(year=year, month=month, day=day_start)\n",
    "end_time = datetime(year=year, month=month, day=day_end)\n",
    "dts = []\n",
    "while start_time < end_time:\n",
    "    dts.append(deepcopy(start_time))\n",
    "    start_time += timedelta(hours=1)\n",
    "data = []\n",
    "for d in tqdm(dts):\n",
    "    data.append(np.load(os.path.join(era5_root, d.strftime('%Y%m%d%H.npy'))))\n",
    "data = np.concatenate(data, axis=0)\n",
    "era5_case_data = torch.from_numpy(data[:, :, :, :]).to(device)\n",
    "era5_case_data = ((era5_case_data - all_means[None,:,None,None]) / all_stds[None,:,None,None])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_case_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
